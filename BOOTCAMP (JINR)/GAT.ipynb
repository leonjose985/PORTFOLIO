{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d1e2b6-1b5c-45bb-8f28-712c2bb16137",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phd\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\torch_geometric\\data\\dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "C:\\Users\\phd\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\torch_geometric\\data\\dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "C:\\Users\\phd\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ogb\\lsc\\pcqm4mv2_pyg.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
     ]
    }
   ],
   "source": [
    "from ogb.lsc.pcqm4mv2_pyg import PygPCQM4Mv2Dataset\n",
    "from ogb.utils import smiles2graph\n",
    "\n",
    "pyg_dataset = PygPCQM4Mv2Dataset(root=\"C://Users//phd//Fernando//ОИЯИ//Model\", smiles2graph=smiles2graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ae0432-2e65-41e9-8ad0-10ed8ffbeef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phd\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ogb\\lsc\\pcqm4mv2_pyg.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  split_dict = replace_numpy_with_torchtensor(torch.load(osp.join(self.root, 'split_dict.pt')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataloader] Tipo de batch 0: <class 'torch_geometric.data.batch.DataBatch'>\n",
      "DataBatch(edge_index=[2, 3678], edge_attr=[3678, 3], x=[1788, 9], y=[128], batch=[1788], ptr=[129])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "split_dict = pyg_dataset.get_idx_split()\n",
    "train_idx = split_dict['train']\n",
    "valid_idx = split_dict['valid']\n",
    "#test_idx = split_dict['test-dev']\n",
    "\n",
    "train_loader = DataLoader(pyg_dataset[train_idx], batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_loader = DataLoader(pyg_dataset[valid_idx], batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "#test_loader = DataLoader(pyg_dataset[test_idx], batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):  # Iterar sobre los batches\n",
    "    print(f\"[Dataloader] Tipo de batch {batch_idx}: {type(batch)}\")  # Ver el tipo\n",
    "    print(batch)  # Opcional: Ver el contenido del batch\n",
    "    break  # Solo imprimimos el primer batch para no llenar la consola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f17e90-8036-4ec4-ba47-c21bc453aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, global_add_pool, global_mean_pool\n",
    "from torch.nn import Linear, Sequential\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "class GATv2_Molecular(torch.nn.Module):\n",
    "    def __init__(self, node_dim=9, edge_dim=3, hidden_dim=256, heads=8,\n",
    "                 n_layers=4, dropout_rate=0.4, use_intermediate_states=True):\n",
    "        super(GATv2_Molecular, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_intermediate_states = use_intermediate_states\n",
    "        \n",
    "        # Embedding inicial\n",
    "        self.node_encoder = AtomEncoder(hidden_dim)\n",
    "        self.edge_encoder = BondEncoder(hidden_dim)\n",
    "        \n",
    "        # Capas GATv2\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.layer_norms = nn.ModuleList()\n",
    "        \n",
    "        # Primera capa GATv2\n",
    "        self.conv_layers.append(\n",
    "            GATv2Conv(\n",
    "                in_channels=hidden_dim,\n",
    "                out_channels=hidden_dim // heads,\n",
    "                heads=heads,\n",
    "                edge_dim=hidden_dim,\n",
    "                dropout=dropout_rate,\n",
    "                concat=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Capas intermedias\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.conv_layers.append(\n",
    "                GATv2Conv(\n",
    "                    in_channels=hidden_dim,\n",
    "                    out_channels=hidden_dim // heads,\n",
    "                    heads=heads,\n",
    "                    edge_dim=hidden_dim,\n",
    "                    dropout=dropout_rate,\n",
    "                    concat=True\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        # Normalización para cada capa\n",
    "        for _ in range(n_layers):\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "            self.layer_norms.append(nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        # Red más profunda para predicción final\n",
    "        pred_input_dim = hidden_dim * 2  # Para mean y sum pooling\n",
    "        if use_intermediate_states:\n",
    "            pred_input_dim += hidden_dim  # Para estados intermedios agregados\n",
    "            \n",
    "        self.prediction_network = Sequential(\n",
    "            Linear(pred_input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def _aggregate_intermediate_states(self, states, batch):\n",
    "        \"\"\"Agrega estados intermedios usando atención\"\"\"\n",
    "        # Convertir lista de estados a tensor\n",
    "        states_tensor = torch.stack([global_mean_pool(state, batch) for state in states])\n",
    "        \n",
    "        # Calcular atención sobre estados\n",
    "        weights = F.softmax(torch.mean(states_tensor, dim=2), dim=0)\n",
    "        \n",
    "        # Aplicar atención y agregar estados\n",
    "        weighted_states = torch.sum(states_tensor * weights.unsqueeze(-1), dim=0)\n",
    "        return weighted_states\n",
    "        \n",
    "    def forward(self, data, return_attention=False):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Embeddings iniciales\n",
    "        x = self.node_encoder(x)\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "        \n",
    "        # Lista para estados intermedios\n",
    "        intermediate_states = []\n",
    "        attention_weights = [] if return_attention else None\n",
    "        \n",
    "        # Propagación a través de capas GATv2\n",
    "        for i in range(self.n_layers):\n",
    "            x_residual = x\n",
    "            \n",
    "            # Aplicar capa GATv2\n",
    "            if return_attention:\n",
    "                x, attention = self.conv_layers[i](x, edge_index, edge_attr, \n",
    "                                                 return_attention_weights=True)\n",
    "                attention_weights.append(attention)\n",
    "            else:\n",
    "                x = self.conv_layers[i](x, edge_index, edge_attr)\n",
    "                \n",
    "            x = self.batch_norms[i](x)\n",
    "            x = self.layer_norms[i](x)\n",
    "            x = F.elu(x)\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "            # Conexión residual\n",
    "            x = x + x_residual\n",
    "            \n",
    "            # Guardar estado intermedio\n",
    "            intermediate_states.append(x)\n",
    "        \n",
    "        # Agregación global combinando mean y sum pooling\n",
    "        x_mean = global_mean_pool(x, batch)\n",
    "        x_sum = global_add_pool(x, batch)\n",
    "        \n",
    "        # Combinar features\n",
    "        features = [x_mean, x_sum]\n",
    "        \n",
    "        # Agregar estados intermedios si está activado\n",
    "        if self.use_intermediate_states:\n",
    "            x_intermediate = self._aggregate_intermediate_states(intermediate_states, batch)\n",
    "            features.append(x_intermediate)\n",
    "        \n",
    "        # Concatenar todas las features\n",
    "        x = torch.cat(features, dim=1)\n",
    "        \n",
    "        # Predicción final usando la red más profunda\n",
    "        x = self.prediction_network(x)\n",
    "        \n",
    "        if return_attention:\n",
    "            return x.squeeze(), attention_weights\n",
    "        return x.squeeze()\n",
    "    \n",
    "    def get_attention_weights(self, data):\n",
    "        \"\"\"\n",
    "        Calcula los pesos de atención solo cuando es necesario.\n",
    "        Retorna una tupla (predicción, pesos de atención)\n",
    "        \"\"\"\n",
    "        return self.forward(data, return_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1974c873-6f49-4ce5-a315-e1447fa6edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GATv2_Molecular(\n",
    "    node_dim=9,  # Dimensión de características de nodos\n",
    "    edge_dim=3,  # Dimensión de características de aristas\n",
    "    hidden_dim=256,  # Dimensión oculta\n",
    "    heads=8,  # Número de cabezas de atención\n",
    "    n_layers=4,  # Número de capas\n",
    "    dropout_rate=0.4  # Tasa de dropout\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "039fe693-00fa-4813-9985-f30f866bc3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def initialize_training(model, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Inicializa el criterio, optimizador y scheduler para el entrenamiento\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.7,\n",
    "        patience=3,\n",
    "        min_lr=1e-4,\n",
    "    )\n",
    "    return criterion, optimizer, scheduler\n",
    "\n",
    "def train_regression(model, data_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Función de entrenamiento por época\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_relative_error = 0\n",
    "    predictions, actual = [], []\n",
    "    \n",
    "    pbar = tqdm(data_loader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Forward pass - GATGNN espera un objeto data directamente\n",
    "        output = model(batch)\n",
    "        \n",
    "        # Asegurar dimensiones correctas\n",
    "        y_true = batch.y.float().view(-1, 1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output.view(-1, 1), y_true)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calcular error relativo porcentual\n",
    "        pred_np = output.detach().cpu().numpy()\n",
    "        true_np = y_true.cpu().numpy()\n",
    "        \n",
    "        # Guardar predicciones para métricas\n",
    "        predictions.extend(pred_np)\n",
    "        actual.extend(true_np)\n",
    "        \n",
    "        # Calcular error relativo\n",
    "        errors = pred_np - true_np\n",
    "        rms_error = np.sqrt(np.mean(np.square(errors)))\n",
    "        mean_y = np.mean(np.abs(true_np))\n",
    "        relative_error_percent = (rms_error / mean_y) * 100 if mean_y != 0 else 0\n",
    "        total_relative_error += relative_error_percent\n",
    "        \n",
    "        # Backward pass con gradient clipping\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Actualizar barra de progreso\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'rel_error': f'{relative_error_percent:.2f}%'\n",
    "        })\n",
    "    \n",
    "    # Calcular métricas finales\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_relative_error = total_relative_error / len(data_loader)\n",
    "    r2 = r2_score(actual, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predictions))\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': avg_loss,\n",
    "        'relative_error': avg_relative_error,\n",
    "        'r2': r2,\n",
    "        'rmse': rmse\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate_regression(model, data_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Función de evaluación\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_relative_error = 0\n",
    "    predictions, actual = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc='Evaluating'):\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(batch)\n",
    "            y_true = batch.y.float().view(-1, 1)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output.view(-1, 1), y_true)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Guardar predicciones\n",
    "            pred_np = output.cpu().numpy()\n",
    "            true_np = y_true.cpu().numpy()\n",
    "            predictions.extend(pred_np)\n",
    "            actual.extend(true_np)\n",
    "            \n",
    "            # Calcular error relativo\n",
    "            errors = pred_np - true_np\n",
    "            rms_error = np.sqrt(np.mean(np.square(errors)))\n",
    "            mean_y = np.mean(np.abs(true_np))\n",
    "            relative_error_percent = (rms_error / mean_y) * 100 if mean_y != 0 else 0\n",
    "            total_relative_error += relative_error_percent\n",
    "    \n",
    "    # Calcular métricas finales\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_relative_error = total_relative_error / len(data_loader)\n",
    "    r2 = r2_score(actual, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predictions))\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': avg_loss,\n",
    "        'relative_error': avg_relative_error,\n",
    "        'r2': r2,\n",
    "        'rmse': rmse,\n",
    "        'predictions': predictions,\n",
    "        'actual': actual\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7d34b43-7ffa-41a1-8a94-5ef9482bd55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, device, \n",
    "                num_epochs=10, early_stopping_patience=5,\n",
    "                learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Función principal de entrenamiento\n",
    "    \"\"\"\n",
    "    # Inicializar componentes de entrenamiento\n",
    "    criterion, optimizer, scheduler = initialize_training(model, learning_rate)\n",
    "    \n",
    "    # Inicializar seguimiento de métricas\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    history = {\n",
    "        'train_loss': [], 'train_relative_error': [], 'train_r2': [], 'train_rmse': [],\n",
    "        'val_loss': [], 'val_relative_error': [], 'val_r2': [], 'val_rmse': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        train_metrics = train_regression(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_metrics = evaluate_regression(model, valid_loader, criterion, device)\n",
    "        \n",
    "        # Actualizar history\n",
    "        for key in ['loss', 'relative_error', 'r2', 'rmse']:\n",
    "            history[f'train_{key}'].append(train_metrics[key])\n",
    "            history[f'val_{key}'].append(val_metrics[key])\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train - Loss: {train_metrics['loss']:.4f}, Rel Error: {train_metrics['relative_error']:.2f}%, R2: {train_metrics['r2']:.4f}, RMSE: {train_metrics['rmse']:.4f}\")\n",
    "        print(f\"Valid - Loss: {val_metrics['loss']:.4f}, Rel Error: {val_metrics['relative_error']:.2f}%, R2: {val_metrics['r2']:.4f}, RMSE: {val_metrics['rmse']:.4f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_metrics['loss'])\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_metrics['loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['loss']\n",
    "            no_improve = 0\n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_val_loss,\n",
    "                'metrics': val_metrics\n",
    "            }, 'best_model.pt')\n",
    "            print(\"Model improved and saved!\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= early_stopping_patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e8a8ebc-f1a5-4daa-a29b-7bfe48afa96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GATGNN training...\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [25:30<00:00, 17.25it/s, loss=0.3129, rel_error=31.53%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:41<00:00, 14.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.2354, Rel Error: 28.03%, R2: 0.8257, RMSE: 0.4852\n",
      "Valid - Loss: 0.1539, Rel Error: 29.33%, R2: 0.9018, RMSE: 0.3918\n",
      "Model improved and saved!\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [25:40<00:00, 17.14it/s, loss=0.1547, rel_error=30.87%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:41<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1329, Rel Error: 28.04%, R2: 0.9016, RMSE: 0.3646\n",
      "Valid - Loss: 0.1322, Rel Error: 29.15%, R2: 0.9158, RMSE: 0.3628\n",
      "Model improved and saved!\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [25:23<00:00, 17.32it/s, loss=0.0936, rel_error=24.30%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:41<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1198, Rel Error: 28.10%, R2: 0.9113, RMSE: 0.3461\n",
      "Valid - Loss: 0.1360, Rel Error: 28.88%, R2: 0.9133, RMSE: 0.3681\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [25:47<00:00, 17.06it/s, loss=0.0731, rel_error=29.97%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:41<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1133, Rel Error: 28.13%, R2: 0.9161, RMSE: 0.3367\n",
      "Valid - Loss: 0.1185, Rel Error: 29.95%, R2: 0.9245, RMSE: 0.3434\n",
      "Model improved and saved!\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [26:05<00:00, 16.86it/s, loss=0.1094, rel_error=22.39%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:41<00:00, 13.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1091, Rel Error: 28.15%, R2: 0.9192, RMSE: 0.3304\n",
      "Valid - Loss: 0.1259, Rel Error: 29.76%, R2: 0.9198, RMSE: 0.3541\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [26:17<00:00, 16.73it/s, loss=0.0667, rel_error=25.05%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:41<00:00, 13.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1063, Rel Error: 28.16%, R2: 0.9213, RMSE: 0.3260\n",
      "Valid - Loss: 0.1092, Rel Error: 29.52%, R2: 0.9305, RMSE: 0.3296\n",
      "Model improved and saved!\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [26:19<00:00, 16.71it/s, loss=0.1386, rel_error=31.35%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:43<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1043, Rel Error: 28.17%, R2: 0.9228, RMSE: 0.3229\n",
      "Valid - Loss: 0.1081, Rel Error: 29.31%, R2: 0.9312, RMSE: 0.3280\n",
      "Model improved and saved!\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [26:37<00:00, 16.52it/s, loss=0.1799, rel_error=29.79%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:43<00:00, 13.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1028, Rel Error: 28.18%, R2: 0.9239, RMSE: 0.3206\n",
      "Valid - Loss: 0.1102, Rel Error: 29.63%, R2: 0.9298, RMSE: 0.3311\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [26:07<00:00, 16.84it/s, loss=0.0713, rel_error=25.42%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:43<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1014, Rel Error: 28.18%, R2: 0.9249, RMSE: 0.3185\n",
      "Valid - Loss: 0.0992, Rel Error: 29.39%, R2: 0.9368, RMSE: 0.3141\n",
      "Model improved and saved!\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [26:08<00:00, 16.83it/s, loss=0.0599, rel_error=27.54%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:43<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1005, Rel Error: 28.19%, R2: 0.9256, RMSE: 0.3170\n",
      "Valid - Loss: 0.1034, Rel Error: 29.36%, R2: 0.9341, RMSE: 0.3208\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [26:07<00:00, 16.84it/s, loss=0.0712, rel_error=27.47%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:43<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0998, Rel Error: 28.19%, R2: 0.9261, RMSE: 0.3159\n",
      "Valid - Loss: 0.1091, Rel Error: 29.62%, R2: 0.9305, RMSE: 0.3296\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [26:13<00:00, 16.77it/s, loss=0.0897, rel_error=24.48%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:43<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0991, Rel Error: 28.20%, R2: 0.9266, RMSE: 0.3148\n",
      "Valid - Loss: 0.1002, Rel Error: 29.30%, R2: 0.9362, RMSE: 0.3158\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [26:10<00:00, 16.81it/s, loss=0.1445, rel_error=25.94%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:43<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0987, Rel Error: 28.20%, R2: 0.9269, RMSE: 0.3142\n",
      "Valid - Loss: 0.1048, Rel Error: 29.27%, R2: 0.9332, RMSE: 0.3230\n",
      "\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [25:48<00:00, 17.04it/s, loss=0.1009, rel_error=25.88%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:43<00:00, 13.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0952, Rel Error: 28.21%, R2: 0.9295, RMSE: 0.3085\n",
      "Valid - Loss: 0.0968, Rel Error: 29.56%, R2: 0.9383, RMSE: 0.3104\n",
      "Model improved and saved!\n",
      "\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [25:45<00:00, 17.08it/s, loss=0.0612, rel_error=23.24%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:43<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0938, Rel Error: 28.22%, R2: 0.9305, RMSE: 0.3063\n",
      "Valid - Loss: 0.0932, Rel Error: 29.57%, R2: 0.9406, RMSE: 0.3046\n",
      "Model improved and saved!\n",
      "\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [25:58<00:00, 16.94it/s, loss=0.0760, rel_error=24.98%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:45<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0904, Rel Error: 28.24%, R2: 0.9331, RMSE: 0.3006\n",
      "Valid - Loss: 0.0886, Rel Error: 29.46%, R2: 0.9436, RMSE: 0.2968\n",
      "\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [25:56<00:00, 16.96it/s, loss=0.0906, rel_error=26.37%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:43<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0900, Rel Error: 28.24%, R2: 0.9334, RMSE: 0.3000\n",
      "Valid - Loss: 0.0892, Rel Error: 29.93%, R2: 0.9432, RMSE: 0.2979\n",
      "\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [25:58<00:00, 16.93it/s, loss=0.0731, rel_error=27.77%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:41<00:00, 13.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0894, Rel Error: 28.24%, R2: 0.9338, RMSE: 0.2990\n",
      "Valid - Loss: 0.0878, Rel Error: 29.75%, R2: 0.9442, RMSE: 0.2954\n",
      "\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 26396/26396 [25:51<00:00, 17.01it/s, loss=0.0762, rel_error=28.05%]\n",
      "Evaluating: 100%|██████████████████████████████████████████████| 575/575 [00:43<00:00, 13.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0870, Rel Error: 28.25%, R2: 0.9355, RMSE: 0.2950\n",
      "Valid - Loss: 0.0847, Rel Error: 29.65%, R2: 0.9461, RMSE: 0.2902\n",
      "Early stopping triggered after 30 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_gatgnn(model, train_loader, valid_loader, device, **kwargs):\n",
    "    print(\"Starting GATGNN training...\")\n",
    "    history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        device=device,\n",
    "        **kwargs\n",
    "    )\n",
    "    return history\n",
    "\n",
    "history = train_gatgnn(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    device=device,\n",
    "    num_epochs=10,\n",
    "    early_stopping_patience=5,\n",
    "    learning_rate=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f06a5-8d50-4045-b754-deb1cac1641e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77198d4-ac4f-4200-8eb9-6b1585d6511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asumiendo que tienes tus datos en variables\n",
    "y_true = ... # Tus valores reales del gap HOMO-LUMO\n",
    "y_pred = ... # Tus predicciones del modelo\n",
    "\n",
    "# Crear la visualización\n",
    "fig = plot_prediction_analysis(y_true, y_pred, save_path='predicciones_homo_lumo.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpu_env]",
   "language": "python",
   "name": "conda-env-gpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
